{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quicker model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow==2.15.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tensor flow dependencies - Functional API\n",
    "\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Model: Lets you build and train a neural network.\n",
    "#Layer: The base class for all Keras layers.\n",
    "#Conv2D: Adds convolutional layers to extract image features.\n",
    "#Dense: Fully connected layer for making decisions.\n",
    "#MaxPooling2D: Reduces image size while keeping key features.\n",
    "#Input: Defines the input shape of the model.\n",
    "#Flatten: Turns multi-dimensional data into a flat vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoiding out of memory errors for memory consumption\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Enable memory growth for each GPU, so TensorFlow uses memory as needed instead of pre-allocating all at once\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folder structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting paths for directories\n",
    "\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect positive and anchor images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/anchor/eb4e3a02-fd81-11ef-bd69-a63d57b1cd1f.jpg'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera resolution: 1280.0x720.0\n",
      "New camera resolution: 1280.0x720.0\n",
      "Capture session complete:\n",
      "- 0 anchor images saved in data/anchor\n",
      "- 0 positive images saved in data/positive\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Define paths for saving images\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [POS_PATH, NEG_PATH, ANC_PATH]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(1)  # Using camera index 1 as you mentioned\n",
    "\n",
    "# Get the maximum available resolution from your camera\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(f\"Camera resolution: {width}x{height}\")\n",
    "\n",
    "# Try setting a higher resolution if your camera supports it\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Verify the new resolution\n",
    "new_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "new_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(f\"New camera resolution: {new_width}x{new_height}\")\n",
    "\n",
    "# Load face detection cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Variables for automatic capture\n",
    "auto_capture = False  # Toggle with 'c' key\n",
    "last_capture_time = 0\n",
    "capture_interval = 0.5  # Time between captures in seconds\n",
    "save_as_anchor = True  # Toggle between anchor and positive\n",
    "num_captured = {'anchor': 0, 'positive': 0}  # Counter for captured images\n",
    "max_images = 300  # Maximum number of images to capture per class\n",
    "\n",
    "# Function to save an image\n",
    "def save_image(frame, save_type):\n",
    "    global num_captured\n",
    "    \n",
    "    if save_type == 'anchor':\n",
    "        path = ANC_PATH\n",
    "        num_captured['anchor'] += 1\n",
    "    else:\n",
    "        path = POS_PATH\n",
    "        num_captured['positive'] += 1\n",
    "        \n",
    "    imgname = os.path.join(path, f'{uuid.uuid1()}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    print(f\"Saved {save_type}: {imgname}, size: {frame.shape[:2]}, Total: {num_captured[save_type]}\")\n",
    "\n",
    "# Main loop\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "    \n",
    "    # Create a copy for display purposes\n",
    "    display_frame = frame.copy()\n",
    "    \n",
    "    # Face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # Status text to display\n",
    "    status_text = f\"Auto-capture: {'ON' if auto_capture else 'OFF'} | \"\n",
    "    status_text += f\"A: {num_captured['anchor']}, P: {num_captured['positive']}\"\n",
    "    if auto_capture:\n",
    "        status_text += f\" | Next: {'Anchor' if save_as_anchor else 'Positive'}\"\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        # Find the largest face\n",
    "        face = max(faces, key=lambda rect: rect[2] * rect[3])\n",
    "        x, y, w, h = face\n",
    "        \n",
    "        # Make the crop square and a bit larger than the detected face\n",
    "        center_x, center_y = x + w//2, y + h//2\n",
    "        size = max(w, h) * 1.5  # 50% larger than the face\n",
    "        size = min(size, new_width, new_height)  # Don't exceed frame boundaries\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        x1 = max(0, int(center_x - size//2))\n",
    "        y1 = max(0, int(center_y - size//2))\n",
    "        x2 = min(int(new_width), int(x1 + size))\n",
    "        y2 = min(int(new_height), int(y1 + size))\n",
    "        \n",
    "        # Draw rectangle for visualization\n",
    "        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Prepare the cropped frame\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Automatic capture logic\n",
    "        current_time = time.time()\n",
    "        if auto_capture and (current_time - last_capture_time) > capture_interval:\n",
    "            # Check if we've reached the maximum number for either class\n",
    "            if (save_as_anchor and num_captured['anchor'] < max_images) or \\\n",
    "               (not save_as_anchor and num_captured['positive'] < max_images):\n",
    "                \n",
    "                # Save the image\n",
    "                save_type = 'anchor' if save_as_anchor else 'positive'\n",
    "                save_image(cropped_frame, save_type)\n",
    "                \n",
    "                # Toggle between anchor and positive for next capture\n",
    "                save_as_anchor = not save_as_anchor\n",
    "                \n",
    "                # Update last capture time\n",
    "                last_capture_time = current_time\n",
    "                \n",
    "            # Check if we've captured enough of both types\n",
    "            elif num_captured['anchor'] >= max_images and num_captured['positive'] >= max_images:\n",
    "                print(f\"Maximum images captured: {max_images} anchors and {max_images} positives\")\n",
    "                auto_capture = False\n",
    "    else:\n",
    "        # If no face detected, use center crop (simpler fallback)\n",
    "        center_x, center_y = int(new_width)//2, int(new_height)//2\n",
    "        size = min(400, int(new_width), int(new_height))  # 400Ã—400 crop or smaller if needed\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        x1 = max(0, center_x - size//2)\n",
    "        y1 = max(0, center_y - size//2)\n",
    "        x2 = min(int(new_width), x1 + size)\n",
    "        y2 = min(int(new_height), y1 + size)\n",
    "        \n",
    "        # Draw rectangle for visualization\n",
    "        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Prepare the cropped frame\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Display status information on screen\n",
    "    cv2.putText(display_frame, status_text, (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "    \n",
    "    # Add usage instructions\n",
    "    instructions = \"a: manual anchor | p: manual positive | c: toggle auto-capture | q: quit\"\n",
    "    cv2.putText(display_frame, instructions, (10, int(new_height) - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Show both the full frame with rectangle and the cropped version\n",
    "    cv2.imshow('Full Frame', display_frame)\n",
    "    \n",
    "    # Only show the cropped frame if it exists\n",
    "    if cropped_frame is not None and cropped_frame.size > 0:\n",
    "        cv2.imshow('Cropped', cropped_frame)\n",
    "    \n",
    "    # Handle keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Manual capture mode\n",
    "    if key == ord('a'):\n",
    "        if len(faces) > 0:  # Only save if a face is detected\n",
    "            save_image(cropped_frame, 'anchor')\n",
    "    \n",
    "    elif key == ord('p'):\n",
    "        if len(faces) > 0:  # Only save if a face is detected\n",
    "            save_image(cropped_frame, 'positive')\n",
    "    \n",
    "    # Toggle automatic capture\n",
    "    elif key == ord('c'):\n",
    "        auto_capture = not auto_capture\n",
    "        print(f\"Auto-capture: {'ON' if auto_capture else 'OFF'}\")\n",
    "        if auto_capture:\n",
    "            last_capture_time = time.time()  # Reset timer\n",
    "    \n",
    "    # Quit\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Capture session complete:\")\n",
    "print(f\"- {num_captured['anchor']} anchor images saved in {ANC_PATH}\")\n",
    "print(f\"- {num_captured['positive']} positive images saved in {POS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting our imaage directories as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchor=tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(200)\n",
    "positive=tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(200)\n",
    "negative=tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify how we create training pairs to ensure equal positive/negative examples\n",
    "positive_samples = min(len(list(anchor)), len(list(positive)))\n",
    "negative_samples = positive_samples\n",
    "anchor = anchor.take(positive_samples)\n",
    "positive = positive.take(positive_samples)\n",
    "negative = negative.take(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    # Add basic image augmentation for better model generalization\n",
    "    img = tf.image.random_flip_left_right(img)  # Random horizontal flip\n",
    "    img = tf.image.random_brightness(img, 0.2)   # Slight brightness adjustment\n",
    "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    # Handle potential numerical instabilities\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives=tf.data.Dataset.zip((anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives=tf.data.Dataset.zip((anchor,negative,tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "\n",
    "# Combine both positive and negative pairs into a single dataset with a mix of matching and non-matching pairs\n",
    "raw_data = positives.concatenate(negatives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split sizes\n",
    "total_size = len(raw_data)\n",
    "train_size = round(total_size * 0.7)\n",
    "val_size = total_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ConcatenateDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data\n",
    "\n",
    "#The first string is our file path to the specific image, second string is the path to either the positive or neggative image\n",
    "#last value determines whether its +ve or -ve for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=raw_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg=sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD, TRAIN AND TEST PARTITION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprcess the input and validation images as twins\n",
    "def preprocess_twins(input_img, validation_img, label):\n",
    "    try:\n",
    "        print(\"Input types:\", type(input_img), type(validation_img), type(label))\n",
    "        print(\"Input image values:\", input_img)\n",
    "        \n",
    "        def process_single_image(img):\n",
    "            # If the image is already a tensor, we need to handle it differently\n",
    "            if isinstance(img, tf.Tensor):\n",
    "                # If it's already a preprocessed image tensor\n",
    "                if img.dtype == tf.float32:\n",
    "                    return tf.image.resize(img,(100,100))\n",
    "                # If it's a string tensor (filepath)\n",
    "                elif img.dtype == tf.string:\n",
    "                    img = tf.io.read_file(img)\n",
    "                    img = tf.io.decode_jpeg(img, channels=3)\n",
    "                    img = tf.cast(img, tf.float32) / 255.0\n",
    "                    img = tf.image.resize(img, (100, 100))\n",
    "                    return img\n",
    "            return None\n",
    "        \n",
    "        processed_input = process_single_image(input_img)\n",
    "        processed_validation = process_single_image(validation_img)\n",
    "        \n",
    "        return processed_input, processed_validation, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing images: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample structure: [<class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'tensorflow.python.framework.ops.EagerTensor'>, <class 'tensorflow.python.framework.ops.EagerTensor'>]\n",
      "First element shape/type: tf.Tensor([], shape=(0,), dtype=int32) <dtype: 'string'>\n"
     ]
    }
   ],
   "source": [
    "# Check the first element of the dataset\n",
    "sample = next(iter(raw_data))\n",
    "print(\"Sample structure:\", [type(x) for x in sample])\n",
    "print(\"First element shape/type:\", tf.shape(sample[0]), sample[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input types: <class 'bytes'> <class 'bytes'> <class 'numpy.float32'>\n",
      "Input image values: b'data/anchor/3373df86-fd40-11ef-bd69-a63d57b1cd1f.jpg'\n"
     ]
    }
   ],
   "source": [
    "res = preprocess_twins(*eg)  # * collecting the eg values from the register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataloader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_pipeline(data, batch_size=32, training=True):\n",
    "    \"\"\"\n",
    "    Creates an optimized data pipeline that properly handles caching.\n",
    "    The order of operations is crucial for efficient data processing.\n",
    "    \"\"\"\n",
    "    # First preprocess the raw images\n",
    "    data = data.map(preprocess_twins, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if training:\n",
    "        # For training data, shuffle before batching\n",
    "        data = data.shuffle(1000)\n",
    "    \n",
    "    # Batch the data\n",
    "    data = data.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    # Cache after batching to store complete batches\n",
    "    data = data.cache()\n",
    "    \n",
    "    # Prefetch at the end for pipeline efficiency\n",
    "    data = data.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shapes\n",
    "def verify_shapes(dataset, name):\n",
    "    print(f\"\\nVerifying {name} shapes:\")\n",
    "    for batch in dataset.take(1):\n",
    "        print(f\"Input shape: {batch[0].shape}\")\n",
    "        print(f\"Validation shape: {batch[1].shape}\")\n",
    "        print(f\"Label shape: {batch[2].shape}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and verify datasets with monitoring\n",
    "def create_and_verify_datasets(raw_data, batch_size=32):\n",
    "    # Calculate split sizes\n",
    "    total_size = len(raw_data)\n",
    "    train_size = round(total_size * 0.7)\n",
    "    \n",
    "    # Split raw data\n",
    "    raw_train = raw_data.take(train_size)\n",
    "    raw_val = raw_data.skip(train_size)\n",
    "    \n",
    "    # Create pipelines\n",
    "    train_data = build_data_pipeline(raw_train, batch_size, training=True)\n",
    "    val_data = build_data_pipeline(raw_val, batch_size, training=False)\n",
    "    \n",
    "    # Verify both datasets\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total examples: {total_size}\")\n",
    "    print(f\"Training examples: {train_size}\")\n",
    "    print(f\"Validation examples: {total_size - train_size}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input types: <class 'tensorflow.python.framework.ops.SymbolicTensor'> <class 'tensorflow.python.framework.ops.SymbolicTensor'> <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Input image values: Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "Input types: <class 'tensorflow.python.framework.ops.SymbolicTensor'> <class 'tensorflow.python.framework.ops.SymbolicTensor'> <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Input image values: Tensor(\"args_0:0\", shape=(), dtype=string)\n",
      "\n",
      "Dataset Statistics:\n",
      "Total examples: 400\n",
      "Training examples: 280\n",
      "Validation examples: 120\n"
     ]
    }
   ],
   "source": [
    "# Use the improved creation function\n",
    "train_data, val_data = create_and_verify_datasets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying Training Data shapes:\n",
      "Input shape: (32, 100, 100, 3)\n",
      "Validation shape: (32, 100, 100, 3)\n",
      "Label shape: (32,)\n",
      "\n",
      "Verifying Validation Data shapes:\n",
      "Input shape: (32, 100, 100, 3)\n",
      "Validation shape: (32, 100, 100, 3)\n",
      "Label shape: (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 07:33:22.627243: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-10 07:33:22.666099: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check both datasets\n",
    "verify_shapes(train_data, \"Training Data\")\n",
    "verify_shapes(val_data, \"Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(32, 100, 100, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 100, 100, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 07:33:22.893091: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_sample =train_sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf  Siememse networks paper\n",
    "\n",
    "#Builds embedding layer\n",
    "\n",
    "def make_embedding():\n",
    "    inp=Input(shape=(100,100,3), name='input_image')\n",
    "\n",
    "    #First block \n",
    "    c1 = Conv2D(64, (10,10), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inp)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    m1 = MaxPooling2D(64,(2,2),padding='same')(c1)\n",
    "    m1 = tf.keras.layers.Dropout(0.3)(m1)\n",
    "\n",
    "    #Second block\n",
    "    c2 = Conv2D(128, (3,3), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64,(2,2),padding='same')(c2)\n",
    "    m2 = tf.keras.layers.Dropout(0.3)(m2)\n",
    "\n",
    "    #Third block \n",
    "    c3 = Conv2D(128, (7,7), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64,(2,2),padding='same')(c3)\n",
    "    m3 = tf.keras.layers.Dropout(0.3)(m3)\n",
    "\n",
    "    #Fouth block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "\n",
    "\n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 91, 91, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 46, 46, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 46, 46, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 44, 44, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 22, 22, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 22, 22, 128)       0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 128)       802944    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 8, 8, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 5, 5, 256)         524544    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4096)              26218496  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27639360 (105.44 MB)\n",
      "Trainable params: 27639232 (105.44 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "\n",
    "    #Init method for inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    #Des the similarity calculation \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "\n",
    "    #Validation image in the network\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    #Combine siamese distance components\n",
    "    siamise_layer = L1Dist()\n",
    "    siamise_layer._name = 'distance'\n",
    "    distances = siamise_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    #Classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_img (InputLayer)      [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " validation_img (InputLayer  [(None, 100, 100, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 4096)                 2763936   ['input_img[0][0]',           \n",
      "                                                          0          'validation_img[0][0]']      \n",
      "                                                                                                  \n",
      " distance (L1Dist)           (None, 4096)                 0         ['embedding[0][0]',           \n",
      "                                                                     'embedding[1][0]']           \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    4097      ['distance[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27643457 (105.45 MB)\n",
      "Trainable params: 27643329 (105.45 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.losses.BinaryCrossentropy()\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=200,\n",
    "    decay_rate=0.97\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule) #learning rate @ 0.0001 initially then gradually reduces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a ckeckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add before training\n",
    "import shutil\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    # Get anchor and positive/negative images\n",
    "    X = batch[:2]\n",
    "    # Get label\n",
    "    Y = batch[2]\n",
    "    \n",
    "    # Record operations with gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the model\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = loss_function(Y, yhat)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    clipped_gradients = [tf.clip_by_value(g, -1.0, 1.0) for g in gradients]\n",
    "    # Apply gradients to update model\n",
    "    opt.apply_gradients(zip(clipped_gradients, siamese_model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1): \n",
    "        print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Track losses for this epoch\n",
    "        epoch_loss = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for idx, batch in enumerate(data):\n",
    "            # Get numerical loss value\n",
    "            loss = train_step(batch)\n",
    "            epoch_loss.update_state(loss)\n",
    "            \n",
    "            # Update progress bar with actual loss value\n",
    "            progbar.update(\n",
    "                idx+1, \n",
    "                values=[('loss', float(epoch_loss.result()))]\n",
    "            )\n",
    "            \n",
    "        # Save checkpoints every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (32, 100, 100, 3)\n",
      "Validation shape: (32, 100, 100, 3)\n",
      "Label shape: (32,)\n",
      "\n",
      "Model input shape:\n",
      "input_img: [(None, 100, 100, 3)]\n",
      "validation_img: [(None, 100, 100, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 07:33:23.765633: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Check data shape\n",
    "for batch in train_data.take(1):\n",
    "    print(\"Input shape:\", batch[0].shape)\n",
    "    print(\"Validation shape:\", batch[1].shape)\n",
    "    print(\"Label shape:\", batch[2].shape)\n",
    "    break\n",
    "\n",
    "# Check model input shape\n",
    "print(\"\\nModel input shape:\")\n",
    "for layer in siamese_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "        print(f\"{layer.name}: {layer.input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 07:33:24.051831: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model by running one batch through it\n",
    "for batch in train_data.take(1):\n",
    "    X = batch[:2]\n",
    "    # Do a forward pass to initialize variables\n",
    "    _ = siamese_model(X, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 56s 7s/step - loss: 0.7638\n",
      "\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 51s 6s/step - loss: 0.6576\n",
      "\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.6056\n",
      "\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 46s 6s/step - loss: 0.6097\n",
      "\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.5556\n",
      "\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 46s 6s/step - loss: 0.5442\n",
      "\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.5228\n",
      "\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.4869\n",
      "\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.4061\n",
      "\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.2856\n",
      "\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 51s 6s/step - loss: 0.1484\n",
      "\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.0827\n",
      "\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0588\n",
      "\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 56s 6s/step - loss: 0.0574\n",
      "\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 52s 7s/step - loss: 0.0281\n",
      "\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 0.0260\n",
      "\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.0226\n",
      "\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0227\n",
      "\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 49s 6s/step - loss: 0.0149\n",
      "\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.0151\n",
      "\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0142\n",
      "\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0154\n",
      "\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0073\n",
      "\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0098\n",
      "\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0054\n",
      "\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.0039\n",
      "\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.0026\n",
      "\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 49s 6s/step - loss: 0.0021\n",
      "\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 51s 6s/step - loss: 0.0023\n",
      "\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 0.0023\n",
      "\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.0021\n",
      "\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 0.0014\n",
      "\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 44s 6s/step - loss: 0.0012\n",
      "\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 44s 6s/step - loss: 0.0015\n",
      "\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 44s 5s/step - loss: 0.0011\n",
      "\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 0.0026\n",
      "\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.0014\n",
      "\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 9.1043e-04\n",
      "\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 0.0019\n",
      "\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 50s 6s/step - loss: 0.0015\n",
      "\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 46s 6s/step - loss: 8.2609e-04\n",
      "\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 6.3093e-04\n",
      "\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 0.0013\n",
      "\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 46s 6s/step - loss: 8.9265e-04\n",
      "\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 45s 6s/step - loss: 7.7539e-04\n",
      "\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 47s 6s/step - loss: 5.8576e-04\n",
      "\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 49s 6s/step - loss: 6.0833e-04\n",
      "\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 5.1257e-04\n",
      "\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 5.3006e-04\n",
      "\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 48s 6s/step - loss: 4.3559e-04\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATE MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
