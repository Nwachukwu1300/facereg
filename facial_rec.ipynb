{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Facial Recognition System: Siamese Network Implementation**\n",
    "\n",
    "This notebook implements a facial verification system using Siamese Neural Networks. The system can determine whether two face images belong to the same person.\n",
    "\n",
    "## Key Components:\n",
    "1. **Data Collection**: Webcam-based face image capture\n",
    "2. **Data Processing**: Creating image pairs for training\n",
    "3. **Model Architecture**: Siamese network with shared weights\n",
    "4. **Training**: Using triplet loss to learn facial embeddings\n",
    "5. **Evaluation**: Testing the model's verification accuracy\n",
    "6. **Deployment**: Saving the model for real-time applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup and dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow==2.15.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tensor flow dependencies - Functional API\n",
    "\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, BatchNormalization\n",
    "\n",
    "\n",
    "#Model: Lets you build and train a neural network.\n",
    "#Layer: The base class for all Keras layers.\n",
    "#Conv2D: Adds convolutional layers to extract image features.\n",
    "#Dense: Fully connected layer for making decisions.\n",
    "#MaxPooling2D: Reduces image size while keeping key features.\n",
    "#Input: Defines the input shape of the model.\n",
    "#Flatten: Turns multi-dimensional data into a flat vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Setup for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable memory growth for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#for multiple gpus\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting Face Images**\n",
    "\n",
    "The system includes a webcam-based data collection script that:\n",
    "\n",
    "1. Captures images from the webcam\n",
    "2. Detects faces using Haar Cascade\n",
    "3. Crops and saves images to appropriate directories (anchor/positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Terms:**\n",
    "- **Anchor Images**: Reference images of the target person\n",
    "- **Positive Images**: Different images of the same person\n",
    "- **Negative Images**: Images of different people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create folder structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting paths for directories\n",
    "\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Collect positive and anchor images using haar cascade model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam Face Capture Implementation\n",
    "\n",
    "This section implements a face detection and capture system with the following features:\n",
    "- Real-time face detection using Haar Cascade\n",
    "- Automatic and manual image capture modes\n",
    "- Intelligent cropping to focus on detected faces\n",
    "- Progress tracking for captured images\n",
    "\n",
    "**Usage Instructions:**\n",
    "- Press 'a' to capture an anchor image\n",
    "- Press 'p' to capture a positive image\n",
    "- Press 'c' to toggle automatic capture mode\n",
    "- Press 'q' to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create directories if they don't exist\n",
    "for directory in [POS_PATH, NEG_PATH, ANC_PATH]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(1)  \n",
    "\n",
    "# Getting the maximum resolution from  camera\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(f\"Camera resolution: {width}x{height}\")\n",
    "\n",
    "# Trying to set a higher resolution \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# verifying the new resolution\n",
    "new_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "new_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(f\"New camera resolution: {new_width}x{new_height}\")\n",
    "\n",
    "# Loading face detection cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Variables for automatic capture\n",
    "auto_capture = False  \n",
    "last_capture_time = 0\n",
    "capture_interval = 0.5  # Time between captures\n",
    "save_as_anchor = True  # Toggle between anchor and positive\n",
    "num_captured = {'anchor': 0, 'positive': 0}  # Counter for captured images\n",
    "max_images = 300  # number of images to capture per class\n",
    "\n",
    "def save_image(frame, save_type):\n",
    "    global num_captured\n",
    "    \n",
    "    if save_type == 'anchor':\n",
    "        path = ANC_PATH\n",
    "        num_captured['anchor'] += 1\n",
    "    else:\n",
    "        path = POS_PATH\n",
    "        num_captured['positive'] += 1\n",
    "        \n",
    "    imgname = os.path.join(path, f'{uuid.uuid1()}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    print(f\"Saved {save_type}: {imgname}, size: {frame.shape[:2]}, Total: {num_captured[save_type]}\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame\")\n",
    "        break\n",
    "    \n",
    "    #vreate a copy for display purposes\n",
    "    display_frame = frame.copy()\n",
    "    \n",
    "    #face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    #status text to show \n",
    "    status_text = f\"Auto-capture: {'ON' if auto_capture else 'OFF'} | \"\n",
    "    status_text += f\"A: {num_captured['anchor']}, P: {num_captured['positive']}\"\n",
    "    if auto_capture:\n",
    "        status_text += f\" | Next: {'Anchor' if save_as_anchor else 'Positive'}\"\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        # find the largest face\n",
    "        face = max(faces, key=lambda rect: rect[2] * rect[3])\n",
    "        x, y, w, h = face\n",
    "        \n",
    "        # make the crop square and a bit larger than the detected face\n",
    "        center_x, center_y = x + w//2, y + h//2\n",
    "        size = max(w, h) * 1.5  # 50% larger than the face\n",
    "        size = min(size, new_width, new_height)  # to make sure we dont pass the boundaries\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        x1 = max(0, int(center_x - size//2))\n",
    "        y1 = max(0, int(center_y - size//2))\n",
    "        x2 = min(int(new_width), int(x1 + size))\n",
    "        y2 = min(int(new_height), int(y1 + size))\n",
    "        \n",
    "        # Draw rectangle for visualization\n",
    "        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Prepare the cropped frame\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        # Automatic capture logic\n",
    "        current_time = time.time()\n",
    "        if auto_capture and (current_time - last_capture_time) > capture_interval:\n",
    "            # Check if we've reached the maximum number for either class\n",
    "            if (save_as_anchor and num_captured['anchor'] < max_images) or \\\n",
    "               (not save_as_anchor and num_captured['positive'] < max_images):\n",
    "                \n",
    "                # Save the image\n",
    "                save_type = 'anchor' if save_as_anchor else 'positive'\n",
    "                save_image(cropped_frame, save_type)\n",
    "                \n",
    "                # Toggle between anchor and positive for next capture\n",
    "                save_as_anchor = not save_as_anchor\n",
    "                \n",
    "                # Update last capture time\n",
    "                last_capture_time = current_time\n",
    "                \n",
    "            # Check if we've captured enough of both types\n",
    "            elif num_captured['anchor'] >= max_images and num_captured['positive'] >= max_images:\n",
    "                print(f\"Maximum images captured: {max_images} anchors and {max_images} positives\")\n",
    "                auto_capture = False\n",
    "    else:\n",
    "        # If no face detected, use center crop (simpler fallback)\n",
    "        center_x, center_y = int(new_width)//2, int(new_height)//2\n",
    "        size = min(400, int(new_width), int(new_height))  # 400×400 crop or smaller if needed\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        x1 = max(0, center_x - size//2)\n",
    "        y1 = max(0, center_y - size//2)\n",
    "        x2 = min(int(new_width), x1 + size)\n",
    "        y2 = min(int(new_height), y1 + size)\n",
    "        \n",
    "        # Draw rectangle for visualization\n",
    "        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Prepare the cropped frame\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Display status information on screen\n",
    "    cv2.putText(display_frame, status_text, (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "    \n",
    "    # Add usage instructions\n",
    "    instructions = \"a: manual anchor | p: manual positive | c: toggle auto-capture | q: quit\"\n",
    "    cv2.putText(display_frame, instructions, (10, int(new_height) - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    # Show both the full frame with rectangle and the cropped version\n",
    "    cv2.imshow('Full Frame', display_frame)\n",
    "    \n",
    "    # Only show the cropped frame if it exists\n",
    "    if cropped_frame is not None and cropped_frame.size > 0:\n",
    "        cv2.imshow('Cropped', cropped_frame)\n",
    "    \n",
    "    # Handle keyboard input\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Manual capture mode\n",
    "    if key == ord('a'):\n",
    "        if len(faces) > 0:  # Only save if a face is detected\n",
    "            save_image(cropped_frame, 'anchor')\n",
    "    \n",
    "    elif key == ord('p'):\n",
    "        if len(faces) > 0:  # Only save if a face is detected\n",
    "            save_image(cropped_frame, 'positive')\n",
    "    \n",
    "    # Toggle automatic capture\n",
    "    elif key == ord('c'):\n",
    "        auto_capture = not auto_capture\n",
    "        print(f\"Auto-capture: {'ON' if auto_capture else 'OFF'}\")\n",
    "        if auto_capture:\n",
    "            last_capture_time = time.time()  # Reset timer\n",
    "    \n",
    "    # Quit\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Capture session complete:\")\n",
    "print(f\"- {num_captured['anchor']} anchor images saved in {ANC_PATH}\")\n",
    "print(f\"- {num_captured['positive']} positive images saved in {POS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting our imaage directories as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchor=tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(200)\n",
    "positive=tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(200)\n",
    "negative=tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify how we create training pairs to ensure equal positive/negative examples\n",
    "positive_samples = min(len(list(anchor)), len(list(positive)))\n",
    "negative_samples = positive_samples\n",
    "anchor = anchor.take(positive_samples)\n",
    "positive = positive.take(positive_samples)\n",
    "negative = negative.take(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Creation\n",
    "\n",
    "Converting our image directories into TensorFlow datasets and creating balanced training pairs:\n",
    "- Equal numbers of positive and negative pairs\n",
    "- Proper train/validation splits\n",
    "- Data augmentation for better generalization\n",
    "\n",
    "## Image Preprocessing Pipeline\n",
    "\n",
    "Implementing preprocessing functions to:\n",
    "- Load and decode images\n",
    "- Apply data augmentation (flips, brightness, contrast)\n",
    "- Resize images to consistent dimensions\n",
    "- Normalize pixel values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    # Add basic image augmentation for better model generalization\n",
    "    img = tf.image.random_flip_left_right(img)  # Random horizontal flip\n",
    "    img = tf.image.random_brightness(img, 0.2)   # Slight brightness adjustment\n",
    "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    # Handle potential numerical instabilities\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both positive and negative pairs into a single dataset with a mix of matching and non-matching pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives=tf.data.Dataset.zip((anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives=tf.data.Dataset.zip((anchor,negative,tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "\n",
    "raw_data = positives.concatenate(negatives) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate split sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(raw_data)\n",
    "train_size = round(total_size * 0.7)\n",
    "val_size = total_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data\n",
    "\n",
    "#The first string is our file path to the specific image, second string is the path to either the positive or neggative image\n",
    "#last value determines whether its +ve or -ve for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=raw_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg=sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pipeline\n",
    "\n",
    "Building an optimized data pipeline with:\n",
    "- Efficient preprocessing with parallel calls\n",
    "- Proper batching and caching\n",
    "- Prefetching for training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD, TRAIN AND TEST PARTITION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprcess the input and validation images as twins\n",
    "def preprocess_twins(input_img, validation_img, label):\n",
    "    try:\n",
    "        print(\"Input types:\", type(input_img), type(validation_img), type(label))\n",
    "        print(\"Input image values:\", input_img)\n",
    "        \n",
    "        def process_single_image(img):\n",
    "            # If the image is already a tensor, we need to handle it differently\n",
    "            if isinstance(img, tf.Tensor):\n",
    "                # If it's already a preprocessed image tensor\n",
    "                if img.dtype == tf.float32:\n",
    "                    return tf.image.resize(img,(100,100))\n",
    "                # If it's a string tensor (filepath)\n",
    "                elif img.dtype == tf.string:\n",
    "                    img = tf.io.read_file(img)\n",
    "                    img = tf.io.decode_jpeg(img, channels=3)\n",
    "                    img = tf.cast(img, tf.float32) / 255.0\n",
    "                    img = tf.image.resize(img, (100, 100))\n",
    "                    return img\n",
    "            return None\n",
    "        \n",
    "        processed_input = process_single_image(input_img)\n",
    "        processed_validation = process_single_image(validation_img)\n",
    "        \n",
    "        return processed_input, processed_validation, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing images: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first element of the dataset\n",
    "sample = next(iter(raw_data))\n",
    "print(\"Sample structure:\", [type(x) for x in sample])\n",
    "print(\"First element shape/type:\", tf.shape(sample[0]), sample[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess_twins(*eg)  # * collecting the eg values from the register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Siamese Network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Embedding Network Architecture\n",
    "\n",
    "The embedding network extracts feature vectors from face images with:\n",
    "- Multiple convolutional blocks with increasing filter sizes\n",
    "- Batch normalization for training stability\n",
    "- Dropout layers to prevent overfitting\n",
    "- Dense output layer producing embedding vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_pipeline(data, batch_size=32, training=True):\n",
    "    \"\"\"\n",
    "    Creates an optimized data pipeline that properly handles caching.\n",
    "    The order of operations is crucial for efficient data processing.\n",
    "    \"\"\"\n",
    "    # First preprocess the raw images\n",
    "    data = data.map(preprocess_twins, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if training:\n",
    "        # For training data, shuffle before batching\n",
    "        data = data.shuffle(1000)\n",
    "    \n",
    "    # Batch the data\n",
    "    data = data.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    # Cache after batching to store complete batches\n",
    "    data = data.cache()\n",
    "    \n",
    "    # Prefetch at the end for pipeline efficiency\n",
    "    data = data.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shapes\n",
    "def verify_shapes(dataset, name):\n",
    "    print(f\"\\nVerifying {name} shapes:\")\n",
    "    for batch in dataset.take(1):\n",
    "        print(f\"Input shape: {batch[0].shape}\")\n",
    "        print(f\"Validation shape: {batch[1].shape}\")\n",
    "        print(f\"Label shape: {batch[2].shape}\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and verify datasets with monitoring\n",
    "def create_and_verify_datasets(raw_data, batch_size=32):\n",
    "    # Calculate split sizes\n",
    "    total_size = len(raw_data)\n",
    "    train_size = round(total_size * 0.7)\n",
    "    \n",
    "    # Split raw data\n",
    "    raw_train = raw_data.take(train_size)\n",
    "    raw_val = raw_data.skip(train_size)\n",
    "    \n",
    "    # Create pipelines\n",
    "    train_data = build_data_pipeline(raw_train, batch_size, training=True)\n",
    "    val_data = build_data_pipeline(raw_val, batch_size, training=False)\n",
    "    \n",
    "    # Verify both datasets\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total examples: {total_size}\")\n",
    "    print(f\"Training examples: {train_size}\")\n",
    "    print(f\"Validation examples: {total_size - train_size}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the improved creation function\n",
    "train_data, val_data = create_and_verify_datasets(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check both datasets\n",
    "verify_shapes(train_data, \"Training Data\")\n",
    "verify_shapes(val_data, \"Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample =train_sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf  Siememse networks paper\n",
    "\n",
    "#Builds embedding layer\n",
    "\n",
    "def make_embedding():\n",
    "    inp=Input(shape=(100,100,3), name='input_image')\n",
    "\n",
    "    #First block \n",
    "    c1 = Conv2D(64, (10,10), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inp)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    m1 = MaxPooling2D(64,(2,2),padding='same')(c1)\n",
    "    m1 = tf.keras.layers.Dropout(0.3)(m1)\n",
    "\n",
    "    #Second block\n",
    "    c2 = Conv2D(128, (3,3), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64,(2,2),padding='same')(c2)\n",
    "    m2 = tf.keras.layers.Dropout(0.3)(m2)\n",
    "\n",
    "    #Third block \n",
    "    c3 = Conv2D(128, (7,7), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64,(2,2),padding='same')(c3)\n",
    "    m3 = tf.keras.layers.Dropout(0.3)(m3)\n",
    "\n",
    "    #Fouth block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "\n",
    "\n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Layer Implementation\n",
    "\n",
    "Custom layer to compute the L1 (Manhattan) distance between embedding vectors:\n",
    "- Takes two embedding vectors as input\n",
    "- Calculates absolute difference between them\n",
    "- Adds small epsilon for numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "\n",
    "    #Init method for inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    #Des the similarity calculation \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Siamese Model\n",
    "\n",
    "The full Siamese architecture:\n",
    "- Two input branches sharing the same embedding network\n",
    "- Distance calculation between embeddings\n",
    "- Final classification layer to output similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "\n",
    "    #Validation image in the network\n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "\n",
    "    #Combine siamese distance components\n",
    "    siamise_layer = L1Dist()\n",
    "    siamise_layer._name = 'distance'\n",
    "    distances = siamise_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    #Classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training Siamese Network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function and Optimizer\n",
    "\n",
    "Setting up the training configuration:\n",
    "- Binary cross-entropy loss for similarity prediction\n",
    "- Adam optimizer with learning rate scheduling\n",
    "- Gradient clipping to prevent exploding gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.losses.BinaryCrossentropy()\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=200,\n",
    "    decay_rate=0.97\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule) #learning rate @ 0.0001 initially then gradually reduces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a ckeckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add before training to check for checkpoint \n",
    "import shutil\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    # Get anchor and positive/negative images\n",
    "    X = batch[:2]\n",
    "    # Get label\n",
    "    Y = batch[2]\n",
    "    \n",
    "    # Record operations with gradient tape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the model\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = loss_function(Y, yhat)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    clipped_gradients = [tf.clip_by_value(g, -1.0, 1.0) for g in gradients]\n",
    "    # Apply gradients to update model\n",
    "    opt.apply_gradients(zip(clipped_gradients, siamese_model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Implementation\n",
    "\n",
    "Custom training loop with:\n",
    "- Epoch-based training with progress tracking\n",
    "- Checkpoint saving for model persistence\n",
    "- Loss monitoring during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1): \n",
    "        print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Track losses for this epoch\n",
    "        epoch_loss = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for idx, batch in enumerate(data):\n",
    "            # Get numerical loss value\n",
    "            loss = train_step(batch)\n",
    "            epoch_loss.update_state(loss)\n",
    "            \n",
    "            # Update progress bar with actual loss value\n",
    "            progbar.update(\n",
    "                idx+1, \n",
    "                values=[('loss', float(epoch_loss.result()))]\n",
    "            )\n",
    "            \n",
    "        # Save checkpoints every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "for batch in train_data.take(1):\n",
    "    print(\"Input shape:\", batch[0].shape)\n",
    "    print(\"Validation shape:\", batch[1].shape)\n",
    "    print(\"Label shape:\", batch[2].shape)\n",
    "    break\n",
    "\n",
    "# Check model input shape\n",
    "print(\"\\nModel input shape:\")\n",
    "for layer in siamese_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "        print(f\"{layer.name}: {layer.input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model by running one batch through it\n",
    "for batch in train_data.take(1):\n",
    "    X = batch[:2]\n",
    "    # Do a forward pass to initialize variables\n",
    "    _ = siamese_model(X, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(train_data, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Evaluating the model using:\n",
    "- Precision: Accuracy of positive predictions\n",
    "- Recall: Ability to find all positive matches\n",
    "- F1 Score: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = raw_data.skip(train_size)\n",
    "test_data = test_data.map(preprocess_twins)\n",
    "test_data = test_data.batch(32)\n",
    "test_data = test_data.prefetch(tf.data.AUTOTUNE)\n",
    "# Create a balanced test dataset with both positive and negative examples\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_data = positives.concatenate(negatives).shuffle(buffer_size=1000)\n",
    "\n",
    "# Split into train/test\n",
    "total_size = len(balanced_data)\n",
    "train_size = round(total_size * 0.7)\n",
    "\n",
    "train_data = balanced_data.take(train_size)\n",
    "test_data = balanced_data.skip(train_size)\n",
    "\n",
    "# Process for the model\n",
    "train_data = train_data.map(preprocess_twins).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_data = test_data.map(preprocess_twins).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    # Initialize metrics\n",
    "    precision_metric = Precision()\n",
    "    recall_metric = Recall()\n",
    "    \n",
    "    print(\"Evaluating model on test data...\")\n",
    "    for batch in test_data:\n",
    "        # Get inputs and labels\n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model(X, training=False)\n",
    "        \n",
    "        # Update metrics\n",
    "        precision_metric.update_state(y, y_pred)\n",
    "        recall_metric.update_state(y, y_pred)\n",
    "    \n",
    "    # Get metric results\n",
    "    precision = precision_metric.result().numpy()\n",
    "    recall = recall_metric.result().numpy()\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(\"\\n===== Model Evaluation =====\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Analysis\n",
    "\n",
    "Finding the optimal decision threshold:\n",
    "- Testing different threshold values\n",
    "- Analyzing precision-recall tradeoffs\n",
    "- Visualizing threshold impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_thresholds(model, test_data):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in test_data:\n",
    "        X = batch[:2]\n",
    "        y = batch[2].numpy()\n",
    "        y_pred = model(X, training=False).numpy().flatten()\n",
    "        \n",
    "        all_preds.extend(y_pred)\n",
    "        all_labels.extend(y)\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = np.arange(0, 0.5, 0.02)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_preds = (np.array(all_preds) > threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.sum((binary_preds == 1) & (np.array(all_labels) == 1))\n",
    "        fp = np.sum((binary_preds == 1) & (np.array(all_labels) == 0))\n",
    "        fn = np.sum((binary_preds == 0) & (np.array(all_labels) == 1))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Find best threshold\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Optimal threshold: {best_threshold:.4f} with F1: {f1_scores[best_idx]:.4f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_threshold(model, test_data, threshold):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in test_data:\n",
    "        X = batch[:2]\n",
    "        y = batch[2].numpy()\n",
    "        y_pred = model(X, training=False).numpy().flatten()\n",
    "        \n",
    "        all_preds.extend(y_pred)\n",
    "        all_labels.extend(y)\n",
    "    \n",
    "    # Apply threshold\n",
    "    binary_preds = (np.array(all_preds) > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = np.sum((binary_preds == 1) & (np.array(all_labels) == 1))\n",
    "    fp = np.sum((binary_preds == 1) & (np.array(all_labels) == 0))\n",
    "    fn = np.sum((binary_preds == 0) & (np.array(all_labels) == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"With threshold {threshold:.4f}:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score = evaluate_model(siamese_model, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = analyze_thresholds(siamese_model, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1 = evaluate_with_threshold(siamese_model, test_data, best_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Visualizing model predictions:\n",
    "- Displaying image pairs with prediction results\n",
    "- Showing distribution of similarity scores\n",
    "- Highlighting the decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, test_data, threshold=0.5, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize prediction results from the Siamese model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Siamese model\n",
    "        test_data: The test dataset\n",
    "        threshold: Decision threshold for positive/negative classification\n",
    "        num_examples: Number of examples to visualize\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get a batch of data\n",
    "    test_batch = next(test_data.as_numpy_iterator())\n",
    "    X = test_batch[:2]  # Input images\n",
    "    y_true = test_batch[2]  # True labels\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = model(X, training=False).numpy().flatten()\n",
    "    \n",
    "    # Limit to the number of examples requested\n",
    "    n = min(num_examples, len(y_true))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Set up the figure\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        \n",
    "        # Display the image pair\n",
    "        axes[0].imshow(X[0][i])\n",
    "        axes[0].set_title('Anchor Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(X[1][i])\n",
    "        \n",
    "        # Determine match status and correctness\n",
    "        is_match = y_true[i] > 0.5\n",
    "        predicted_match = y_pred[i] > threshold\n",
    "        is_correct = (is_match == predicted_match)\n",
    "        \n",
    "        # Create detailed title\n",
    "        title = f\"{'Match' if is_match else 'No Match'} (Ground Truth)\\n\"\n",
    "        title += f\"Prediction: {'Match' if predicted_match else 'No Match'}\\n\"\n",
    "        title += f\"Confidence: {y_pred[i]:.4f}, Threshold: {threshold:.2f}\"\n",
    "        \n",
    "        # Set the title color based on match/no-match, not correctness\n",
    "        color = 'green' if is_match else 'red'\n",
    "        \n",
    "        axes[1].set_title(title, color=color)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)  # Make room for the text\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(siamese_model, test_data, threshold=best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_threshold_analysis(model, test_data):\n",
    "    \"\"\"\n",
    "    Visualize the impact of different thresholds on model performance\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Siamese model\n",
    "        test_data: The test dataset\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Collect all predictions\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in test_data:\n",
    "        X = batch[:2]\n",
    "        y = batch[2].numpy()\n",
    "        y_pred = model(X, training=False).numpy().flatten()\n",
    "        \n",
    "        all_preds.extend(y_pred)\n",
    "        all_labels.extend(y)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Compute metrics at different thresholds\n",
    "    thresholds = np.arange(0, 1.01, 0.05)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_preds = (all_preds > threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.sum((binary_preds == 1) & (all_labels == 1))\n",
    "        fp = np.sum((binary_preds == 1) & (all_labels == 0))\n",
    "        fn = np.sum((binary_preds == 0) & (all_labels == 1))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    # Plot metrics vs threshold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precisions, 'b-', label='Precision')\n",
    "    plt.plot(thresholds, recalls, 'g-', label='Recall')\n",
    "    plt.plot(thresholds, f1_scores, 'r-', label='F1 Score')\n",
    "    \n",
    "    # Mark the optimal threshold\n",
    "    plt.axvline(x=best_threshold, color='k', linestyle='--', \n",
    "                label=f'Best Threshold = {best_threshold:.2f}')\n",
    "    \n",
    "    plt.title('Model Performance vs. Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot prediction distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Separate predictions for positive and negative pairs\n",
    "    pos_preds = all_preds[all_labels == 1]\n",
    "    neg_preds = all_preds[all_labels == 0]\n",
    "    \n",
    "    # Plot histograms\n",
    "    plt.hist(pos_preds, bins=20, alpha=0.6, color='green', label='Matching Pairs')\n",
    "    plt.hist(neg_preds, bins=20, alpha=0.6, color='red', label='Non-matching Pairs')\n",
    "    \n",
    "    plt.axvline(x=best_threshold, color='k', linestyle='--', \n",
    "                label=f'Best Threshold = {best_threshold:.2f}')\n",
    "    \n",
    "    plt.title('Distribution of Similarity Scores')\n",
    "    plt.xlabel('Model Prediction (Similarity Score)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_threshold_analysis(siamese_model, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Saving and Deployment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "\n",
    "Saving the trained model with:\n",
    "- Full model architecture and weights\n",
    "- Embedding model for feature extraction\n",
    "- Metadata including optimal threshold\n",
    "- Usage examples for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, embedding_model, best_threshold, model_metrics, version=None):\n",
    "    \"\"\"\n",
    "    Save the Siamese model with metadata\n",
    "    \n",
    "    Args:\n",
    "        model: The trained Siamese model\n",
    "        embedding_model: The embedding part of the model\n",
    "        best_threshold: The optimal threshold for classification\n",
    "        model_metrics: Dictionary with precision, recall, and f1_score\n",
    "        version: Optional version string (defaults to timestamp)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Create directories\n",
    "    base_dir = 'models'\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create version name with timestamp if not provided\n",
    "    if not version:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        version = f\"siamese_model_{timestamp}\"\n",
    "    \n",
    "    # Create model directory\n",
    "    model_dir = os.path.join(base_dir, version)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save full model\n",
    "    full_model_path = os.path.join(model_dir, 'full_model')\n",
    "    model.save(full_model_path)\n",
    "    print(f\"Full model saved to: {full_model_path}\")\n",
    "    \n",
    "    # 2. Save embedding model\n",
    "    embedding_path = os.path.join(model_dir, 'embedding')\n",
    "    embedding_model.save(embedding_path)\n",
    "    print(f\"Embedding model saved to: {embedding_path}\")\n",
    "    \n",
    "    # 3. Save model weights\n",
    "    weights_path = os.path.join(model_dir, 'model_weights.h5')\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"Model weights saved to: {weights_path}\")\n",
    "    \n",
    "    # 4. Save model metadata\n",
    "    metadata = {\n",
    "        'model_name': version,\n",
    "        'date_created': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'framework_version': tf.__version__,\n",
    "        'image_size': [100, 100, 3],\n",
    "        'metrics': {\n",
    "            'precision': float(model_metrics['precision']),\n",
    "            'recall': float(model_metrics['recall']),\n",
    "            'f1_score': float(model_metrics['f1_score'])\n",
    "        },\n",
    "        'optimal_threshold': float(best_threshold),\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, 'model_info.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"Model metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # 5. Create usage example file\n",
    "    example_code = f\"\"\"# Example code to use the Siamese face verification model\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define L1Dist layer (needed for model loading)\n",
    "class L1Dist(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "\n",
    "# Load the saved model\n",
    "model_path = '{full_model_path}'\n",
    "model = tf.keras.models.load_model(\n",
    "    model_path, \n",
    "    custom_objects={{'L1Dist': L1Dist}}\n",
    ")\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    # Resize to model input size\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    # Normalize pixel values\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def verify_faces(img_path1, img_path2, threshold={best_threshold}):\n",
    "    # Preprocess images\n",
    "    img1 = preprocess_image(img_path1)\n",
    "    img2 = preprocess_image(img_path2)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img2 = np.expand_dims(img2, axis=0)\n",
    "    \n",
    "    # Get prediction\n",
    "    result = model.predict([img1, img2])\n",
    "    similarity_score = result[0][0]\n",
    "    \n",
    "    # Determine if match based on optimal threshold\n",
    "    is_match = similarity_score > threshold\n",
    "    \n",
    "    return {{'is_match': bool(is_match), 'score': float(similarity_score)}}\n",
    "\n",
    "# Example usage\n",
    "# result = verify_faces('person1.jpg', 'person2.jpg')\n",
    "# print(f\"Match: {{result['is_match']}}, Score: {{result['score']:.4f}}\")\n",
    "\"\"\"\n",
    "    \n",
    "    example_path = os.path.join(model_dir, 'usage_example.py')\n",
    "    with open(example_path, 'w') as f:\n",
    "        f.write(example_code)\n",
    "    print(f\"Usage example saved to: {example_path}\")\n",
    "    \n",
    "    print(f\"\\nModel successfully saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model_metrics = {\n",
    "#     'precision': precision,\n",
    "#     'recall': recall, \n",
    "#     'f1_score': f1_score\n",
    "# }\n",
    "# \n",
    "# saved_dir = save_model(\n",
    "#     model=siamese_model,\n",
    "#     embedding_model=embedding,\n",
    "#     best_threshold=best_threshold,\n",
    "#     model_metrics=model_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall, \n",
    "    'f1_score': f1\n",
    "}\n",
    "\n",
    "saved_dir = save_model(\n",
    "    model=siamese_model,\n",
    "    embedding_model=embedding,\n",
    "    best_threshold=best_threshold,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
